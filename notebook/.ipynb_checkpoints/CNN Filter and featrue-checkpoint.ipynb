{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model  \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last layer\n",
    "model = VGG16(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get VGG-16 Layer Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1 (3, 3, 3, 64)\n",
      "block1_conv2 (3, 3, 64, 64)\n",
      "block2_conv1 (3, 3, 64, 128)\n",
      "block2_conv2 (3, 3, 128, 128)\n",
      "block3_conv1 (3, 3, 128, 256)\n",
      "block3_conv2 (3, 3, 256, 256)\n",
      "block3_conv3 (3, 3, 256, 256)\n",
      "block4_conv1 (3, 3, 256, 512)\n",
      "block4_conv2 (3, 3, 512, 512)\n",
      "block4_conv3 (3, 3, 512, 512)\n",
      "block5_conv1 (3, 3, 512, 512)\n",
      "block5_conv2 (3, 3, 512, 512)\n",
      "block5_conv3 (3, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Get all filter layers name\n",
    "# Convolutional layers name likes \"block#_conv#\"\n",
    "for layer in model.layers:\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    \n",
    "    # Get filter weights\n",
    "    filters, bias = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "filters, biases = model.layers[1].get_weights()\n",
    "\n",
    "# normalize filter value from 0-1 so can visualize it \n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters-f_min) / (f_max - f_min)\n",
    "\n",
    "# We only dispaly first 8\n",
    "n_filters, ix = 8, 1\n",
    "\n",
    "#fig = pyplot.figure(figsize=(20, 12))\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    \n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # Plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Feature Map\n",
    "\n",
    "The activation maps, called feature maps, capture the result of applying the filters to input, such as the input image or another feature map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import expand_dims\n",
    "\n",
    "img_path = '../py/img_train/pos/2_A.jpg'\n",
    "\n",
    "\n",
    "def draw_feature_map(img_path):\n",
    "    img_resize = (1024, 1024)\n",
    "\n",
    "    img = image.load_img(img_path, target_size=img_resize)\n",
    "\n",
    "    img = img_to_array(img)\n",
    "    img = expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    # Get feature map for the first hidden layer\n",
    "    feature_maps = model.predict(img)\n",
    "    fig = pyplot.figure(figsize=(20, 16))\n",
    "\n",
    "    square = 8\n",
    "    ix = 1\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn off axis\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # plot filter channel in grayscale\n",
    "            pyplot.imshow(feature_maps[0, :, :, ix-1])\n",
    "            ix += 1\n",
    "\n",
    "    # show the figure\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_feature_map('../py/img_train/pos/2_A.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_feature_map('../py/img_train/pos/2_B.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_feature_map('../py/img_train/neg/0_A.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_feature_map('../py/img_train/neg/0_B.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
